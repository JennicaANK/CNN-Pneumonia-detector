{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55eebfb",
   "metadata": {},
   "source": [
    "# Pneumonia Detection Project - Model Construction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627d4f3",
   "metadata": {},
   "source": [
    "## Pneumonia Detection from Chest X-Ray Images  \n",
    "### Model Construction & Model Evalution\n",
    "\n",
    "This notebook focuses on constructing a CNN model from scratch and utilizing a pre-trained  ResNet18 model for our pneumonia detection project in CS 171.  \n",
    "We will:\n",
    "- Construct a baseline CNN from scratch with three convolutional blocks and ReLU activations.\n",
    "- Optimize using Adam optimizer, with early stopping and learning rate scheduling.\n",
    "- Evaluate results using test accuracy, confusion matrix, and ROC-AUC score.\n",
    "- Fine-tune a pretrained ResNet18 model from torchvision.models for performance comparison.\n",
    "- Analyze the effect of transfer learning on accuracy and convergence.\n",
    "- Use Grad-CAM to visualize activation maps and interpret the model’s focus areas.\n",
    "\n",
    "Authors: **Aye Nyein Kyaw** and **Isiah Ketton**  \n",
    "Course: **CS 171 – Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d86a34",
   "metadata": {},
   "source": [
    "- Import PyTorch and `torchvision` utilities.\n",
    "- Define the paths to the training, validation, and test directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844257ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f6cbb",
   "metadata": {},
   "source": [
    "# CNN Construction\n",
    "\n",
    "Very baseline work in progress models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ae4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 18 * 18, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5f690",
   "metadata": {},
   "source": [
    "## ResNet18 Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might need further work to work with grayscale images\n",
    "def build_resnet18():\n",
    "    model = resnet18(weights=\"IMAGENET1K_V1\")\n",
    "    \n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    model.fc = nn.Linear(512, 2)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs171",
   "language": "python",
   "name": "cs171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
